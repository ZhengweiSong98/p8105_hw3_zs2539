---
title: "HW3"
output: github_document
---

## Problem 1 Solution

#### Importing data
```{r}
library(p8105.datasets)
data("instacart")

tail(instacart)
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` variables. The variables include `r colnames(instacart)`. These variables record unique numeric identifiers like ID about user and product, and descriptive character observations about name and category.

#### a.
```{r}
library(tidyverse)
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```

There are `r length(unique(pull(instacart, aisle)))` aisles. _fresh vegetables_, _fresh fruits_ and _packaged vegetables fruits_ are the most items ordered from, all of which have more than **78000** observations.

#### b.
```{r}
library(ggridges)
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  ggplot(aes(x = aisle, y = n)) + geom_point() + theme(axis.text.x = element_text
  (angle = 90)) +
  labs(
    title = "The number of items ordered in each aisle",
    x = "Aisles",
    y = "Items Ordered"
  )
```

#### c.
```{r}
instacart %>% 
    filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
    group_by(aisle) %>% 
    count(product_name) %>% 
    mutate(rank = min_rank(desc(n))) %>% 
    filter(rank <= 3) %>% 
    arrange(aisle, rank) %>% 
    knitr::kable()
```


#### d.
```{r}
instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name ==  "Coffee Ice Cream") %>%
  group_by(product_name, order_dow) %>% 
  summarize(
   mean_hour = mean(order_hour_of_day)
  ) %>% 
   mutate(
     order_dow = recode(order_dow, "0" = "Sun", "1" = "Mon", "2" = "Tue", "3" = "Wed", "4" = "Thu", "5" = "Fri", "6" = "Sat"),
     mean_hour = round(mean_hour, 2)
     ) %>% 
    pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
   knitr::kable()
```

## Problem 2 Solution

#### a. Load and tidy the data
```{r}
accel =
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_counts") %>% 
  mutate(
    week_section = recode(day,
      "Friday" = "Weekday",
      "Monday" = "Weekday",
      "Tuesday" = "Weekday",
      "Wednesday" = "Weekday",
      "Thursday" = "Weekday",
      "Saturday" = "Weekend",
      "Sunday" = "Weekend"
    )
  )
```

There are `r nrow(accel)` observations and `r ncol(accel)` variables, including information about week, day (id, name, type), minute and the activity counts.

#### b. Create a total activity variable for each day and a table showing these totals
```{r}
accel_day =
accel %>% 
  group_by(day_id, day, week_section) %>% 
  summarize(
    day_counts = sum(activity_counts)
  )
  
knitr::kable(accel_day)
```

##### Then create a barplot of activity counts itself and by day type

```{r}
accel_day %>%
  group_by(day_id) %>%
  ggplot(aes(x = day_id, y = day_counts, fill = day_id)) + geom_bar(stat = "identity", position = "dodge") + labs(
    title = "Accelerometer Daily Activity Counts",
    x = "Day No.",
    y = "Activity Counts"
  )
```

```{r}
accel_day %>%
  group_by(day) %>%
  ggplot(aes(x = day, y = day_counts, fill = day)) + geom_bar(stat = "identity", position = "dodge") + labs(
    title = "Accelerometer Daily Activity Counts by day type",
    x = "Day of the week",
    y = "Activity Counts"
  )
```

Trends:

By the one form and two plots above, some apparent trends are as follows.

* On the last several days, activity counts tend to be lower than that in the beginning days.
* Tuesday recorded the lowest activity counts, while Monday recorded the highest counts.

#### c. Create a single-panel plot that shows the 24-hour activity time courses for each day
```{r}
accel %>% 
  group_by(day_id, day, week) %>%
  ggplot(aes(x = as.numeric(minute), y = activity_counts, color = day, group = day_id)) + 
  stat_smooth(se = FALSE, method = "loess") +
  labs(title = "24-hour Activity Counts",
       x = "Hour of a Day",
       y = "Activity counts") +
  scale_x_continuous(
    breaks = seq(60, 1440, 60), 
    labels = as.character(c(1:24))) +
  scale_y_continuous(trans = "log")
```

Conclusions:

* 9am to 7pm recorded the highest activity counts per day for the 5 weeks, while other time slots recorded low counts, possibly because of sleeping time.
* Most curves are similar, with pinnacle usually occurred in the afternoon.


## Problem 3 Solution

#### Importing data
```{r}
library(p8105.datasets)
data("ny_noaa")
```

There are `r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` variables, including information about precipitation, snowfall, snow depth and temperature range by weather station ID and by date. Missing data is apparent in this dataset where `r sum(is.na(ny_noaa))` blanks missing, consisting of `r sum(is.na(ny_noaa$prcp))` in _precipitation_, `r sum(is.na(ny_noaa$snow))` in _snowfall_, `r sum(is.na(ny_noaa$snwd))` in _snow depth_, `r sum(is.na(ny_noaa$tmax))` in _tmax_, `r sum(is.na(ny_noaa$tmin))` in _tmin_.

#### a. Cleaning Data
```{r}
ny_precipitation = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(
      month = as.numeric(month),
      year = as.numeric(year),
      
      prcp = as.numeric(prcp),
      tmax = as.numeric(tmax),
      tmin = as.numeric(tmin),
         
      prcp = prcp / 10,
      tmin = tmin / 10,
      tmax = tmax / 10
      )

ny_precipitation %>% 
  count(snow) %>% 
  arrange(desc(n))
```

The most commonly observed values is 0 for snowfall, which is reasonable that snowfall only occurs during cold seasons in New York.

#### b. Create a two-panel plot showing the average max temperature in January and in July in each station across years
```{r}
  ny_precipitation %>%
  select(id, year, month, tmax) %>% 
  filter(month ==  c(1,7)) %>% 
  group_by(id, year, month) %>% 
  mutate(
    mean_tmax = mean(tmax, na.rm = T),
    month = month.name[month]
    ) %>% 
    drop_na(mean_tmax) %>%
  ggplot(aes(x = year, y = mean_tmax, color = month)) +
  geom_point(size = .3, alpha = .5) +
   geom_smooth(se = F) +
  labs(
    title = "Average maximum tempreture of weather stations between January and July in New York",
    x = "Year",
    y = "Average Monthly Tempreture (Â°C)") + 
  theme(legend.position = "none")  +
  facet_grid(~month) +
  viridis::scale_color_viridis(discrete = T, option = "plasma") +
    scale_color_manual(values=c("blue", "red"))
```

In general, the range of maximum temperature is from -10 to 10 degrees Celsius in January, while from 22 to 32 in July. Also, in January, the maximum temperature fluctuates apparently compared with that in July. There is some apparent outliers such as one station extremely warmer on Jan. 2004, while one station extremely colder July 1988.


#### c. (i) tmax - tmin plot for the full dataset
```{r}
  ny_precipitation %>% 
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_hex()
```

#### c. (ii) plotting distribution of snowfall values 0-100 by year
```{r}
ny_precipitation %>% 
  filter(snow < 100 & snow > 0) %>%
  mutate(year = factor(year)) %>%
  ggplot(aes(x = year, y = snow)) + 
  geom_violin(aes(fill = year)) +
    stat_summary(fun = "median", color = "blue") +
  theme(axis.text.x = element_text(angle = 90),
        legend.position="none")
```

In general, min and max temperature have a positive correlation by hex plot. Also, their binary temperature ranges from -5 - 0 to 17 - 30 degrees Celsius. By the second violin plot, we can see the snowfall values do not vary apparently by year, and their mean snowfall are all approximately 25mm.
